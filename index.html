<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>DeepHPC by Saharati90</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">DeepHPC</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/Saharati90/DeepHPC" class="btn">View on GitHub</a>
      <a href="https://github.com/Saharati90/DeepHPC/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/Saharati90/DeepHPC/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="high-performance-computing-in-deep-learning" class="anchor" href="#high-performance-computing-in-deep-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>High-Performance Computing in Deep Learning</h1>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>Machine learning is an iterative experimental process. The common methodology in solving a problem involves a preliminary idea, upon which the first solution is developed. After building the proposed solution and performing experiments on the solution, more often than not one finds himself redesigning the architecture in an effort to overcome the shortcomings of the original solution. 
To illustrate the point, imagine that we want to build a speech recognition system capable of deciphering uttered letters in a wave file. One of the first solutions that may come to mind would be a multilayer neural network similar to the one depicted in the figure below. </p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/simple_network2.png" alt="simple network"> </p>

<p><em>Picture credits: Andrew Ng</em></p>

<p>After implementing one such architecture and evaluating the results, one may notice that refining the network to a recurrent neural network instead of a simple network may be better able to capture the temporal dependency of the uttered letters and hence improve the performance of the system. So the architecture is transformed into a network similar to figure below.</p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/rnn.png" alt="RNN"> </p>

<p><em>Picture credits: Andrew Ng</em></p>

<p>This process of constantly implementing solutions, experimenting with them and then coming up with a redesign is very time consuming. A good portion of the time invested in this process goes towards running experiments with typically large data. Using techniques from HPC, and running experiments on large clusters is bound to reduce the timing of this iterative process and give the engineer the ability to try more novel solutions in shorter timeframes. </p>

<p>Given any computational model we also have the intuition that increasing the training data, we can achieve higher accuracies. But it is suggested [1] that there is an upper bound on the added performance when just increasing the volume of training data and after some point, one has to use more complicated learning techniques. In this work, we want to verify this hypothesis by testing different machine learning architectures on a dataset, increasing the volume of the data to a point where no significant improvement is resulted, and then improving the machine learning architecture each time. As the experiment time would become prohibitive, we will set-up a high-performance computing environment for performing the experiments.</p>

<h2>
<a id="methodology" class="anchor" href="#methodology" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Methodology</h2>

<p>After cleaning up the data, we will apply a simple one layer neural network to the data. The optimization is performed using a Logloss cost function. We start by a small batch of data, and gradually increase the size of the training data up to a point when no significant increase in the performance is noted. </p>

<p>At this point, we will increase the complexity of our model, by iteratively changing our architecture to one that includes many more hidden layers. Each time the amount of data that will produce the optimum accuracy is noted, along with performance metrics discussed in our Metrics section. </p>

<p>To be able to note the effect of using larger volumes of data, we will set up a high performance computing center and balance the load on as many GPUs as our budget allows. This would make the experimenting phase possible within a reasonable time frame. </p>

<h3>
<a id="tensorflow" class="anchor" href="#tensorflow" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TensorFlow</h3>

<p>TensorFlow is an open source software library for numerical computation using data flow graphs in which nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. </p>

<p>TensorFlow is Google Brain's second generation machine learning system, with a reference implementation released as open source software last November. It provides a Python API, as well as a less documented C/C++ API.</p>

<p>TensorFlow can run on multiple CPUs and GPUs (with optional CUDA extensions for general-purpose computing on graphics processing units). It runs on 64-bit Linux or Mac OS X desktop or server systems, as well as on mobile computing platforms, including Android and Apple's iOS. </p>

<p>The purpose is to train neural networks to detect and decipher patterns and correlations. There are some other libraries for numerical computations in python. One of these libraries is Theano, which is one of the strong python libraries for machine learning tasks. However, Theano's applicability in different GPU's is not experimentally verified yet, which was one of the major reasons for pushing us through TensorFlow, which gives strong computational functions to use in neural networks like activation functions, calculating loss and gradient descent.</p>

<h2>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

<p>As a global specialist in personal insurance, <a href="https://www.bnpparibascardif.com/">BNP Paribas Cardif</a> serves 90 million clients in 36 countries across Europe, Asia and Latin America.
In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.<a href="https://www.kaggle.com/c/bnp-paribas-cardif-claims-management">(Kaggle Competition)</a></p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/Kagglepic1.PNG" alt="kaggle"></p>

<p>BNP Paribas Cardif is providing an anonymized database with two categories of claims:</p>

<ol>
<li>claims for which approval could be accelerated leading to faster payments</li>
<li>claims for which additional information is required before approval</li>
</ol>

<p>Working on this dataset, we want to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore, provide a better service to its customers.This dataset contains 114322 claims with 131 features. </p>

<p>After fixing the pipeline of the project and achieving reasonable results on this dataset, we can use larger ones for further investigations. </p>

<h2>
<a id="evaluation" class="anchor" href="#evaluation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evaluation</h2>

<p>In our project we want to implement different architectures of neural networks with different complexities on a large dataset to spot the effect of complexity of neural networks on its accuracy on large datasets. There is an assumption in machine learning which says simple machine architectures don't learn from extra provided data. However, if we increase the complexity of machine then the performance would increase by the increase in amount of data. It is very important to consider that it is not applicable to train very complex neural networks on very large datasets, since it needs a lot of time. Therefore, it is very important to take advantage of some high performance computing techniques like using multiple GPUs/CPUs to split the dataset into different batches and load these batches to the designed complex neural network while it is training on some other data batches.</p>

<p>In this project we aim to measure the strong/weak scaling and the speed up of different neural network architectures to evaluate the effect of using hpc techniques in complex neural networks and measure the accuracy of different architectures on large datasets to evaluate how complexity affects neural network's learning and accuracy. To measure the accuracy of the network we will use logloss function which is generally used to measure the logarithm of the likelihood function for a Bernouli random distribution.</p>

<h2>
<a id="time-table" class="anchor" href="#time-table" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Time Table</h2>

<p>The time scheduling for this project is as follows:</p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/Screen%20Shot%202016-03-25%20at%207.41.08%20PM.png" alt="TimeTable"> </p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<ul>
<li>Mohsen Salari (<a href="https://github.com/mohsensalari" class="user-mention">@mohsensalari</a>), First Year PhD Student, Biomedical Informatics Department, Emory University</li>
<li>Pooya Mobadersany (<a href="https://github.com/pooya-mobadersany" class="user-mention">@pooya-mobadersany</a>), First Year PhD Student, Biomedical Informatics Department, Emory University</li>
<li>Sahar Harati (<a href="https://github.com/saharati90" class="user-mention">@saharati90</a>), Second Year PhD Student, Computer Science Department, Emory University</li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/Saharati90/DeepHPC">DeepHPC</a> is maintained by <a href="https://github.com/Saharati90">Saharati90</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
