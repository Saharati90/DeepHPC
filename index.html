<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>DeepHPC by Saharati90</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">DeepHPC</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/Saharati90/DeepHPC" class="btn">View on GitHub</a>
      <a href="https://github.com/Saharati90/DeepHPC/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/Saharati90/DeepHPC/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="high-performance-computing-in-deep-learning" class="anchor" href="#high-performance-computing-in-deep-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>High-Performance Computing in Deep Learning</h1>

<h2>
<a id="team-members" class="anchor" href="#team-members" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Team Members</h2>

<ul>
<li>Sahar Harati (<a href="https://github.com/saharati90" class="user-mention">@saharati90</a>), Second Year PhD Student, Computer Science Department, Emory University</li>
<li>Mohsen Salari (<a href="https://github.com/mohsensalari" class="user-mention">@mohsensalari</a>), First Year PhD Student, Biomedical Informatics Department, Emory University</li>
<li>Pooya Mobadersany (<a href="https://github.com/pooya-mobadersany" class="user-mention">@pooya-mobadersany</a>), First Year PhD Student, Biomedical Informatics Department, Emory University</li>
</ul>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>Machine learning is an iterative experimental process. The common methodology in solving a problem involves a preliminary idea, upon which the first solution is developed. After building the proposed solution and performing experiments on the solution, more often than not one finds himself redesigning the architecture in an effort to overcome the shortcomings of the original solution. 
To illustrate the point, imagine that we want to build a speech recognition system capable of deciphering uttered letters in a wave file. One of the first solutions that may come to mind would be a multilayer neural network similar to the one depicted in the figure below. </p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/simple_network2.png" alt="simple network"> </p>

<p><em>Picture credits: Andrew Ng</em></p>

<p>After implementing one such architecture and evaluating the results, one may notice that refining the network to a recurrent neural network instead of a simple network may be better able to capture the temporal dependency of the uttered letters and hence improve the performance of the system. So the architecture is transformed into a network similar to figure below.</p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/rnn.png" alt="RNN"> </p>

<p><em>Picture credits: Andrew Ng</em></p>

<p>This process of constantly implementing solutions, experimenting with them and then coming up with a redesign is very time consuming. A good portion of the time invested in this process goes towards running experiments with typically large data. Using techniques from HPC, and running experiments on large clusters is bound to reduce the timing of this iterative process and give the engineer the ability to try more novel solutions in shorter timeframes. </p>

<p>Given any computational model we also have the intuition that increasing the training data, we can achieve higher accuracies. But it is suggested [1] that there is an upper bound on the added performance when just increasing the volume of training data and after some point, one has to use more complicated learning techniques. In this work, we want to verify this hypothesis by testing different machine learning architectures on a dataset, increasing the volume of the data to a point where no significant improvement is resulted, and then improving the machine learning architecture each time. As the experiment time would become prohibitive, we will set-up a high performance computing environment for performing the experiments.</p>

<h2>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

<p>As a global specialist in personal insurance, <a href="https://www.bnpparibascardif.com/">BNP Paribas Cardif</a> serves 90 million clients in 36 countries across Europe, Asia and Latin America.
In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.</p>

<p><img src="https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/Kagglepic1.PNG" alt="kaggle"></p>

<p>BNP Paribas Cardif is providing an anonymized database with two categories of claims:</p>

<ol>
<li>claims for which approval could be accelerated leading to faster payments</li>
<li>claims for which additional information is required before approval</li>
</ol>

<p>Working on this dataset, we want to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore, provide a better service to its customers.This dataset contains 114322 claims with 131 features. There are lots of missing </p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>You can <a href="https://help.github.com/articles/basic-writing-and-formatting-syntax/#mentioning-users-and-teams" class="user-mention">@mention</a> a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/Saharati90/DeepHPC">DeepHPC</a> is maintained by <a href="https://github.com/Saharati90">Saharati90</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
