{
  "name": "DeepHPC",
  "tagline": "",
  "body": "#High-Performance Computing in Deep Learning\r\n\r\nTeam Members\r\n-----\r\n* Sahar Harati (@saharati90), Second Year PhD Student, Computer Science Department, Emory University\r\n* Mohsen Salari (@mohsensalari), First Year PhD Student, Biomedical Informatics Department, Emory University\r\n* Pooya Mobadersany (@pooya-mobadersany), First Year PhD Student, Biomedical Informatics Department, Emory University\r\n\r\n##Introduction\r\nMachine learning is an iterative experimental process. The common methodology in solving a problem involves a preliminary idea, upon which the first solution is developed. After building the proposed solution and performing experiments on the solution, more often than not one finds himself redesigning the architecture in an effort to overcome the shortcomings of the original solution. \r\nTo illustrate the point, imagine that we want to build a speech recognition system capable of deciphering uttered letters in a wave file. One of the first solutions that may come to mind would be a multilayer neural network similar to the one depicted in the figure below. \r\n\r\n![simple network](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/simple_network2.png) \r\n\r\n_Picture credits: Andrew Ng_\r\n\r\nAfter implementing one such architecture and evaluating the results, one may notice that refining the network to a recurrent neural network instead of a simple network may be better able to capture the temporal dependency of the uttered letters and hence improve the performance of the system. So the architecture is transformed into a network similar to figure below.\r\n\r\n![RNN](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/rnn.png) \r\n\r\n_Picture credits: Andrew Ng_\r\n\r\n\r\nThis process of constantly implementing solutions, experimenting with them and then coming up with a redesign is very time consuming. A good portion of the time invested in this process goes towards running experiments with typically large data. Using techniques from HPC, and running experiments on large clusters is bound to reduce the timing of this iterative process and give the engineer the ability to try more novel solutions in shorter timeframes. \r\n\r\nGiven any computational model we also have the intuition that increasing the training data, we can achieve higher accuracies. But it is suggested [1] that there is an upper bound on the added performance when just increasing the volume of training data and after some point, one has to use more complicated learning techniques. In this work, we want to verify this hypothesis by testing different machine learning architectures on a dataset, increasing the volume of the data to a point where no significant improvement is resulted, and then improving the machine learning architecture each time. As the experiment time would become prohibitive, we will set-up a high performance computing environment for performing the experiments.\r\n## Data\r\nAs a global specialist in personal insurance, [BNP Paribas Cardif](https://www.bnpparibascardif.com/) serves 90 million clients in 36 countries across Europe, Asia and Latin America.\r\nIn a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.\r\n\r\n![kaggle](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/Kagglepic1.PNG)\r\n\r\nBNP Paribas Cardif is providing an anonymized database with two categories of claims:\r\n\r\n1. claims for which approval could be accelerated leading to faster payments\r\n2. claims for which additional information is required before approval\r\n\r\nWorking on this dataset, we want to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore, provide a better service to its customers.This dataset contains 114322 claims with 131 features. There are lots of missing \r\n\r\n\r\n\r\n\r\n### Authors and Contributors\r\nYou can @mention a GitHub username to generate a link to their profile. The resulting `<a>` element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (@defunkt), PJ Hyett (@pjhyett), and Tom Preston-Werner (@mojombo) founded GitHub.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}