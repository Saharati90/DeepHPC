{
  "name": "DeepHPC",
  "tagline": "",
  "body": "#High-Performance Computing in Deep Learning\r\n\r\n##Introduction\r\nMachine learning is an iterative experimental process. The common methodology in solving a problem involves a preliminary idea, upon which the first solution is developed. After building the proposed solution and performing experiments on the solution, more often than not one finds himself redesigning the architecture in an effort to overcome the shortcomings of the original solution. \r\nTo illustrate the point, imagine that we want to build a speech recognition system capable of deciphering uttered letters in a wave file. One of the first solutions that may come to mind would be a multilayer neural network similar to the one depicted in the figure below. \r\n\r\n![simple network](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/simple_network2.png) \r\n\r\n_Picture credits: Andrew Ng_\r\n\r\nAfter implementing one such architecture and evaluating the results, one may notice that refining the network to a recurrent neural network instead of a simple network may be better able to capture the temporal dependency of the uttered letters and hence improve the performance of the system. So the architecture is transformed into a network similar to figure below.\r\n\r\n![RNN](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/rnn.png) \r\n\r\n_Picture credits: Andrew Ng_\r\n\r\n\r\nThis process of constantly implementing solutions, experimenting with them and then coming up with a redesign is very time consuming. A good portion of the time invested in this process goes towards running experiments with typically large data. Using techniques from HPC, and running experiments on large clusters is bound to reduce the timing of this iterative process and give the engineer the ability to try more novel solutions in shorter timeframes. \r\n\r\nGiven any computational model we also have the intuition that increasing the training data, we can achieve higher accuracies. But it is suggested [1] that there is an upper bound on the added performance when just increasing the volume of training data and after some point, one has to use more complicated learning techniques. In this work, we want to verify this hypothesis by testing different machine learning architectures on a dataset, increasing the volume of the data to a point where no significant improvement is resulted, and then improving the machine learning architecture each time. As the experiment time would become prohibitive, we will set-up a high-performance computing environment for performing the experiments.\r\n##Methodology\r\nAfter cleaning up the data, we will apply a simple one layer neural network to the data. The optimization is performed using a Logloss cost function. We start by a small batch of data, and gradually increase the size of the training data up to a point when no significant increase in the performance is noted. \r\n\r\nAt this point, we will increase the complexity of our model, by iteratively changing our architecture to one that includes many more hidden layers. Each time the amount of data that will produce the optimum accuracy is noted, along with performance metrics discussed in our Metrics section. \r\n\r\nTo be able to note the effect of using larger volumes of data, we will set up a high performance computing center and balance the load on as many GPUs as our budget allows. This would make the experimenting phase possible within a reasonable time frame. \r\n###TensorFlow\r\nTensorFlow is an open source software library for numerical computation using data flow graphs in which nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. \r\n\r\nTensorFlow is Google Brain's second generation machine learning system, with a reference implementation released as open source software last November. It provides a Python API, as well as a less documented C/C++ API.\r\n\r\nTensorFlow can run on multiple CPUs and GPUs (with optional CUDA extensions for general-purpose computing on graphics processing units). It runs on 64-bit Linux or Mac OS X desktop or server systems, as well as on mobile computing platforms, including Android and Apple's iOS. \r\n\r\nThe purpose is to train neural networks to detect and decipher patterns and correlations. There are some other libraries for numerical computations in python. One of these libraries is Theano, which is one of the strong python libraries for machine learning tasks. However, Theano's applicability in different GPU's is not experimentally verified yet, which was one of the major reasons for pushing us through TensorFlow, which gives strong computational functions to use in neural networks like activation functions, calculating loss and gradient descent.\r\n## Data\r\nAs a global specialist in personal insurance, [BNP Paribas Cardif](https://www.bnpparibascardif.com/) serves 90 million clients in 36 countries across Europe, Asia, and Latin America.\r\nIn a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.[(Kaggle Competition)](https://www.kaggle.com/c/bnp-paribas-cardif-claims-management)\r\n\r\n![kaggle](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/Kagglepic1.PNG)\r\n\r\nBNP Paribas Cardif is providing an anonymized database with two categories of claims:\r\n\r\n1. claims for which approval could be accelerated leading to faster payments\r\n2. claims for which additional information is required before approval\r\n\r\nWorking on this dataset, we want to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore, provide a better service to its customers.This dataset contains 114322 claims with 131 features. \r\n\r\nAfter fixing the pipeline of the project and achieving reasonable results on this dataset, we can use larger ones for further investigations.\r\n\r\n###Missing Values: \r\n99.9% of NA's are most likely not missing at random, so imputing them with medians or means wouldn't help, and we treated them as information units. We also applied rounding to numerical variables, such as round(x,5). There is random noise [-1;1]*1e-7 added to each numeric variable.\r\n\r\n###Feature Selection and Extraction: \r\nMany of the features in the dataset are not independent features which mean they have no more information than other and are useless. We had to select a feature set which has some evident predictive power. Since we still haven't the Amazon cloud set yet, we couldn't use the whole training set to see which features are more informative and important than others. We plan to use \"[Boruta](https://m2.icm.edu.pl/boruta/)\" package which is an all-relevant wrapper feature selection method that currently takes weeks to run on our 4 cores machines. We could run it on a small portion of the dataset but since we are trying to get the most accurate prediction utilizing HPC, it's preferred to use the whole dataset. While Boruta is generally a sequential algorithm, the underlying random forest classifier is a trivially parallel task and thus, Boruta can be distributed even over hundreds of cores, provided that a parallel version of the random forest algorithm is used. \r\n\r\n![figs](https://github.com/Saharati90/DeepHPC/blob/master/images/data.png?raw=true)\r\n\r\n_Picture credits: Kaggle Competition Forum_\r\n\r\n###Categorical Features\r\nWe simply counted the number of unique categories in each categorical feature and replaced each categorical value with an associated integer. \r\n\r\n\r\n##Evaluation\r\n\r\nIn our project we want to implement different architectures of neural networks with different complexities on a large dataset to spot the effect of complexity of neural networks on its accuracy on large datasets. There is an assumption in machine learning which says simple machine architectures don't learn from extra provided data. However, if we increase the complexity of machine then the performance would increase by the increase in amount of data. It is very important to consider that it is not applicable to train very complex neural networks on very large datasets, since it needs a lot of time. Therefore, it is very important to take advantage of some high performance computing techniques like using multiple GPUs/CPUs to split the dataset into different batches and load these batches to the designed complex neural network while it is training on some other data batches.\r\n\r\n In this project we aim to measure the strong/weak scaling and the speed up of different neural network architectures to evaluate the effect of using hpc techniques in complex neural networks and measure the accuracy of different architectures on large datasets to evaluate how complexity affects neural network's learning and accuracy. To measure the accuracy of the network we will use logloss function which is generally used to measure the logarithm of the likelihood function for a Bernouli random distribution.\r\n##Time Table\r\nThe time scheduling for this project is as follows:\r\n\r\n![TimeTable](https://raw.githubusercontent.com/Saharati90/DeepHPC/master/images/Screen Shot 2016-03-25 at 7.41.08 PM.png) \r\n\r\nHere is the brief description for works which are done:\r\n\r\n1) TensorFlow is installed and setup.\r\n\r\n2) TensorFlow classes and functions which are useful in developing the network are developed and learned.\r\n\r\n3) Tools for providing visual results in TensorFlow are learned.\r\n\r\n4) To get familiar with TensorFlow a simple classification model with softmax layer is implemented and trained.\r\n\r\nFollowing is the architecture of this simple classifier:\r\n\r\n\r\n![tens2](https://github.com/Saharati90/DeepHPC/blob/master/images/1.png?raw=true)\r\n\r\n_Picture credits: www.tensorflow.com_\r\n\r\nIn which y = softmax(wx + b)\r\n\r\nTo update the weights and biases in this model we used gradient descent optimizer to minimize the cross entropy as the cost function of the model.\r\n\r\n### Authors and Contributors\r\n* Mohsen Salari (@mohsensalari), First Year PhD Student, Biomedical Informatics Department, Emory University\r\n* Pooya Mobadersany (@pooya-mobadersany), First Year PhD Student, Biomedical Informatics Department, Emory University\r\n* Sahar Harati (@saharati90), Second Year PhD Student, Computer Science Department, Emory University",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}